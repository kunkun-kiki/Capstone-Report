{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This project attempts to classify SBA Loan's using Random Forest Classification and MLPClassifier\nBelow are the necessary imports I will be using.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy import mean\nfrom numpy import std\nfrom pandas import read_csv\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-16T02:11:44.695032Z","iopub.execute_input":"2022-10-16T02:11:44.695369Z","iopub.status.idle":"2022-10-16T02:11:44.706006Z","shell.execute_reply.started":"2022-10-16T02:11:44.695338Z","shell.execute_reply":"2022-10-16T02:11:44.705094Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/sba-loans-case-data-set/SBAcase.11.13.17.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Reading and Cleaning data for Feature Selection\nThe data set is read into a dataframe using pandas. All non-numeric features are removed. ChgOffDate mostly consistes of NA and BalanceGross is all zeros, so both are removed. The features are placed extracted to X and default dummy variables to y. The total dataset loan default rate is printed below. The feature set is then standarized and normalized.\n<br />\nTODO: encode and add non-numeric features. Handle NA values. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nscaler = StandardScaler()\nscaler1 = MinMaxScaler()\n\n\ndf = pd.read_csv(\"../input/sba-loans-case-data-set/SBAcase.11.13.17.csv\")\n\ndf = df.select_dtypes(exclude = 'object')\n#removes 'ChgOffDate' and 'BalanceGross' features\ndf = df.loc[:, df.columns != 'ChgOffDate']\ndf = df.loc[:, df.columns != 'BalanceGross']\ndf = df.loc[:, df.columns != 'Zip']\n#remove remaining NA values\ndf = df.dropna()\ndf.info()\nx_train, x_test, y_train, y_test = train_test_split(df.iloc[:,0:df.shape[1]-1], df.iloc[:,df.shape[1]-1], test_size = .2, random_state = 1)\nx_train = StandardScaler().fit_transform(x_train)\nx_test = StandardScaler().fit_transform(x_test)\n\nprint('Loan Default percentage in dataset: %.1f%%' % (100*np.mean(y_train)))\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T02:11:46.187783Z","iopub.execute_input":"2022-10-16T02:11:46.188105Z","iopub.status.idle":"2022-10-16T02:11:46.267857Z","shell.execute_reply.started":"2022-10-16T02:11:46.188075Z","shell.execute_reply":"2022-10-16T02:11:46.266970Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 2098 entries, 0 to 2100\nData columns (total 24 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Selected           2098 non-null   int64  \n 1   LoanNr_ChkDgt      2098 non-null   int64  \n 2   NAICS              2098 non-null   int64  \n 3   ApprovalDate       2098 non-null   int64  \n 4   ApprovalFY         2098 non-null   int64  \n 5   Term               2098 non-null   int64  \n 6   NoEmp              2098 non-null   int64  \n 7   NewExist           2098 non-null   float64\n 8   CreateJob          2098 non-null   int64  \n 9   RetainedJob        2098 non-null   int64  \n 10  FranchiseCode      2098 non-null   int64  \n 11  UrbanRural         2098 non-null   int64  \n 12  DisbursementDate   2098 non-null   float64\n 13  DisbursementGross  2098 non-null   int64  \n 14  ChgOffPrinGr       2098 non-null   int64  \n 15  GrAppv             2098 non-null   int64  \n 16  SBA_Appv           2098 non-null   int64  \n 17  New                2098 non-null   int64  \n 18  RealEstate         2098 non-null   int64  \n 19  Portion            2098 non-null   float64\n 20  Recession          2098 non-null   int64  \n 21  daysterm           2098 non-null   int64  \n 22  xx                 2098 non-null   float64\n 23  Default            2098 non-null   int64  \ndtypes: float64(4), int64(20)\nmemory usage: 409.8 KB\nLoan Default percentage in dataset: 33.6%\n(1678, 23)\n(1678,)\n(420, 23)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nscaler = StandardScaler()\nscaler1 = MinMaxScaler()\n\n\ndf = pd.read_csv(\"../input/sba-loans-case-data-set/SBAcase.11.13.17.csv\")\n\ndf = df.select_dtypes(exclude = 'object')\n#removes 'ChgOffDate' and 'BalanceGross' features\ndf = df.loc[:, df.columns != 'ChgOffDate']\ndf = df.loc[:, df.columns != 'BalanceGross']\ndf = df.loc[:, df.columns != 'Zip']\n#remove remaining NA values\ndf = df.dropna()\ndefault = 0\npif = 0\nfor i, v in df.items():\n    if i == \"Default\":\n        for k in v:\n            if k == 0:\n                pif += 1\n            else:\n                default += 1\n\nprint(\"Data amount\", df.shape)\nprint(\"Default: \", default, \"PIF: \", pif)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T02:11:50.997575Z","iopub.execute_input":"2022-10-16T02:11:50.997909Z","iopub.status.idle":"2022-10-16T02:11:51.030175Z","shell.execute_reply.started":"2022-10-16T02:11:50.997877Z","shell.execute_reply":"2022-10-16T02:11:51.029357Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Data amount (2098, 24)\nDefault:  686 PIF:  1412\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Feature Selection using ExtraTreesClassifier\nThe Extremely Randomized Trees Classifier was used for feature selection. This model uses ensembled decision trees (in this case 100) to calculate the Gini impurity of each feature. This is unncessary, with not a lot of features all of them can be used. In this case, my impurity condition keeps all features.\n<br />\nThe feature impurity is graphed below.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\n\n# Build a forest and compute the impurity-based feature importances\nforest = ExtraTreesClassifier(random_state=1)\n\nforest.fit(x_train, y_train)\n\n# feat_importances = pd.Series(forest.feature_importances_, index=x_train.columns)\n# feat_importances.nlargest(10).plot(kind='barh')\n# plt.show()\n\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# # lists to store feature indices and impurity value are created\n# feature_indices = []\n# feature_importance = []\n\n# # Print the feature ranking\n# print(\"Feature ranking:\")\n\n# for f in range(X.shape[1]):\n#     print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n#     feature_indices.append(indices[f])\n#     feature_importance.append(importances[indices[f]])\n# feature_importance_array = np.array(list(zip(feature_indices,feature_importance)))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), indices)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()\n\n# mean = feature_importance_array.mean(axis=0)[1]\n\n# best_features = feature_importance_array[(feature_importance_array >= 0.00).all(axis=1)]\n","metadata":{"execution":{"iopub.status.busy":"2022-10-16T02:11:51.856414Z","iopub.execute_input":"2022-10-16T02:11:51.856741Z","iopub.status.idle":"2022-10-16T02:11:52.288355Z","shell.execute_reply.started":"2022-10-16T02:11:51.856711Z","shell.execute_reply":"2022-10-16T02:11:52.287297Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"### The selected features are chosen from dataframe\nThe features chosen above (in this case all of them), are extracted from the features dataframe (X) and converted to an array for modeling (X1). \n","metadata":{}},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n\n# scaler = StandardScaler()\n# scaler1 = MinMaxScaler()\n\n\n\n# feat_list = []\n# for i in range(0, best_features.shape[0]):\n#     feat_list.append(int(best_features[i,0]))\n\n\n# important_features_df = df.iloc[:,feat_list]\n\n# new_X = important_features_df.iloc[:,0:best_features.shape[0]]\n# new_X = new_X.astype(float)\n\n# new_X2 = scaler.fit_transform(new_X)\n# new_X3 = scaler1.fit_transform(new_X2)\n\n# X1 = np.array(new_X3)\n\n# print(X1.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T02:11:32.374959Z","iopub.execute_input":"2022-10-16T02:11:32.375299Z","iopub.status.idle":"2022-10-16T02:11:32.379732Z","shell.execute_reply.started":"2022-10-16T02:11:32.375264Z","shell.execute_reply":"2022-10-16T02:11:32.378360Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### RandomForestClassifier model is trained and tested, Low Accuracy\nThe data was split into training and test sets. 10 fold cross validation was used to ensure genrality in the model. I experimented with better accuracy measures than model.predict, and included a confusion matrix. I am still having trouble with this model being too accurate and I'm not sure why. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, log_loss, f1_score\nfrom sklearn.dummy import DummyClassifier\nimport seaborn as sns\n\ndummy_model = DummyClassifier(strategy = \"most_frequent\", random_state = 1)\nmodel = RandomForestClassifier(random_state = 1)\n# Model optimized from RandomSearchcv (below), no better than default:\n# model = RandomForestClassifier(max_depth=43, max_features='log2', min_samples_split=152, n_estimators=162, random_state=1)\n\n\n# evaluate the model\ncv = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1)\nn_scores = cross_val_score(model, x_train, y_train, scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='balanced_accuracy')\nprint('Training set k-fold cross validation accuracy: %.3f%% (%.3f)' % (100*np.mean(n_scores), np.std(n_scores)))\n\n# report performance\ndummy_model.fit(x_train,y_train)\ndummy_acc = dummy_model.score(x_test,y_test)*100\ndumb_pred = dummy_model.predict(x_test)\nprint('Dummy model accuracy: %.3f%%' % dummy_acc)\n\nmodel.fit(x_train,y_train)\n\n#basic predict\ny_pred = model.predict(x_test)\npred_accuracy_percentage = 100 * accuracy_score(y_test,y_pred)\nprint('Test set accuracy: %.3f%%' % pred_accuracy_percentage)\n\n#ROC AUC metric\npred_auc = model.predict_proba(x_test)[:,1]\nacc = roc_auc_score(y_test, pred_auc)\nprint('AUC: %.3f%%' % (100*acc))\n\n#log loss metric\nlloss = log_loss(y_test,pred_auc, normalize = True)\nprint('Log loss: %.3f' % lloss)\n\n#f1_score\nf1 = f1_score(y_test,y_pred)\nprint('F1 Score: %.3f' % f1)\n\n#Confusion Matrix\nplt.figure()\nax = plt.axes()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot = True, fmt = 'd', ax = ax)\nax.set_title('Confusion Matrix of Loan Default Classifier')\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomizedSearch RandomForest, n_estimators, max_features, max_depth, min_samples_split\nThis doesn't help much. Needs more work or RandomForest may not be a good method.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\n\nforest = RandomForestClassifier(random_state = 1)\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 1, stop = 200, num = 199)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2',None]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 200, num = 199)]\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(start = 1, stop = 200, num = 199)]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\n# pprint(random_grid)\n\n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=1, n_jobs = -1, scoring='balanced_accuracy', return_train_score=True)\n# Fit the random search model\nrf_random.fit(x_train, y_train)\n\ncvres2 = rf_random.cv_results_\nfor mean_score, params in zip(cvres2['mean_test_score'], cvres2[\"params\"]):\n    print(np.sqrt(mean_score), params)\n    \nprint(\"best estimator\")\nprint(rf_random.best_estimator_)\n\nprint(\"best params\")\nprint(rf_random.best_params_)\nprint(\"best estimator\")\nprint(rf_random.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, log_loss, f1_score\nfrom sklearn.dummy import DummyClassifier\nimport seaborn as sns\n\ndummy_model = DummyClassifier(strategy = \"most_frequent\", random_state = 1)\nmodel = RandomForestClassifier(random_state = 1)\n# Model optimized from RandomSearchcv (below), no better accuracy than default:\n#model = RandomForestClassifier(max_depth=43, max_features='log2', min_samples_split=152, n_estimators=162, random_state=1)\n\n\n# evaluate the model\ncv = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1)\nn_scores = cross_val_score(model, x_train, y_train, scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='balanced_accuracy')\nprint('Training set k-fold cross validation accuracy: %.3f%% (%.3f)' % (100*np.mean(n_scores), np.std(n_scores)))\n\n# report performance\ndummy_model.fit(x_train,y_train)\ndummy_acc = dummy_model.score(x_test,y_test)*100\ndumb_pred = dummy_model.predict(x_test)\nprint('Dummy model accuracy: %.3f%%' % dummy_acc)\n\nmodel.fit(x_train,y_train)\n\n#basic predict\ny_pred = model.predict(x_test)\npred_accuracy_percentage = 100 * accuracy_score(y_test,y_pred)\nprint('Test set accuracy: %.3f%%' % pred_accuracy_percentage)\n\n#ROC AUC metric\npred_auc = model.predict_proba(x_test)[:,1]\nacc = roc_auc_score(y_test, pred_auc)\nprint('AUC: %.3f%%' % (100*acc))\n\n#log loss metric\nlloss = log_loss(y_test,pred_auc, normalize = True)\nprint('Log loss: %.3f' % lloss)\n\n#f1_score\nf1 = f1_score(y_test,y_pred)\nprint('F1 Score: %.3f' % f1)\n\n#Confusion Matrix\nplt.figure()\nax = plt.axes()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot = True, fmt = 'd', ax = ax)\nax.set_title('Confusion Matrix of Loan Default Classifier')\nplt.show()\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple MLPClassifier\nI have finally got a nerual network training on this data accurately! Standardizing and normalizing the features drastically increased my accuracy. This model needs more work, I am excited to keep playing with this an incorporate HyperOpt tuning, graphing, and more.","metadata":{}},{"cell_type":"markdown","source":"### Sequential ANN, accuracies and epochs graphed with validation set, basic regularization added","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.metrics import BinaryCrossentropy\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, log_loss, f1_score\nfrom keras.regularizers import l2\n\nx_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size = .2, random_state = 1)\nl = l2(0.003)\n\n\ndef getModel():\n    den = Dense(15, kernel_regularizer=l, bias_regularizer=l,kernel_initializer='glorot_uniform', activation='relu')\n    model = Sequential()\n    model.add(Dense(15, kernel_regularizer=l, bias_regularizer=l,input_dim=23, kernel_initializer='glorot_uniform', activation='relu'))\n    model.add(den)\n    model.add(den)\n    model.add(Dense(1, kernel_regularizer=l, bias_regularizer=l,kernel_initializer='glorot_uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\n\nmodel = getModel()\nhistory = model.fit(x_train_, y_train_, validation_data=(x_val, y_val), epochs=1000, verbose=0)\n# plot history\n# summarize history for accuracy\nplt.figure()\nplt.ylabel('accuracy', fontsize = 14)\nplt.xlabel('epochs', fontsize = 14)\nplt.title(\"Learning curve of epochs\", fontsize = 14)\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\nplt.show()\n\n\n\nprint('Dummy model accuracy: %.3f%%' % dummy_acc)\n#basic predict\ny_pred = model.predict_classes(x_test)\npred_accuracy_percentage = 100 * accuracy_score(y_test,y_pred)\nprint('Test set accuracy: %.3f%%' % pred_accuracy_percentage)\n\n#ROC AUC metric\npred_auc = model.predict_proba(x_test)[:,0]\nacc = roc_auc_score(y_test, pred_auc)\nprint('AUC: %.3f%%' % (100*acc))\n\n#log loss metric\nlloss = log_loss(y_test,pred_auc, normalize = True)\nprint('Log loss: %.3f' % lloss)\n\n#f1_score\nf1 = f1_score(y_test,y_pred)\nprint('F1 Score: %.3f' % f1)\n\n#Confusion Matrix\nplt.figure()\nax = plt.axes()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot = True, fmt = 'd', ax = ax)\nax.set_title('Confusion Matrix of Loan Default Classifier')\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grid Search CV Batch size, epochs, init, optimizer (TAKES FOREVER TO RUN)","metadata":{}},{"cell_type":"code","source":"# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import GridSearchCV\n# import numpy\n\n# # Function to create model, required for KerasClassifier\n# def create_model(optimizer='rmsprop', init='glorot_uniform'):\n# # create model\n#     model = Sequential()\n#     model.add(Dense(12, input_dim=23, kernel_initializer=init, activation='relu'))\n#     model.add(Dense(8, kernel_initializer=init, activation='relu'))\n#     model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n#     # Compile model\n#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n#     return model\n\n# # create model\n# model = KerasClassifier(build_fn=create_model, verbose=0)\n# # grid search epochs, batch size and optimizer\n# optimizers = ['rmsprop', 'adam']\n# inits = ['glorot_uniform', 'normal', 'uniform']\n# epochs = [50, 100, 150]\n# batches = [5, 10, 20]\n# param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n# grid_result = grid.fit(x_train, y_train)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sklearn MLPClassifier learning rate with f1 of training rates graphed\nSeaborn is used to graph the change in f1 over increasing training set sizes. The graph seems to show there may be an overfitting problem.","metadata":{}},{"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, ShuffleSplit\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.neural_network import MLPClassifier\n\nx_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size = .2, random_state = 1)\n\nbigtrain_sizes = list(range(1,1073))\ntrain_sizes = [1]\nfor i in bigtrain_sizes:\n    if i % 100 == 0:\n        train_sizes.append(int(i))\ntrain_sizes.append(1073)\n\nclf = MLPClassifier(batch_size=10, verbose=False, early_stopping=True)\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\ntrain_sizes, train_scores, validation_scores = learning_curve(estimator= clf, X = x_train_, y = y_train_, train_sizes = train_sizes, cv = cv, scoring = 'f1', verbose = 0)\ntrain_scores_mean = train_scores.mean(axis =1)\nvalidation_scores_mean = validation_scores.mean(axis = 1)\n\n\nplt.style.use('seaborn')\nplt.plot(train_sizes, train_scores_mean, label = \"Training Error\")\nplt.plot(train_sizes, validation_scores_mean, label = \"Validation Error\")\nplt.ylabel('f1', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title(\"Learning curves\", fontsize = 14)\nplt.legend()\nplt.ylim(0,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation Curve used for simple hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"import warnings  \nwarnings.filterwarnings('ignore')\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, ShuffleSplit\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.neural_network import MLPClassifier\n\nx_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size = .2, random_state = 1)\n# print(x_train_.shape)\n# print(x_train.shape)\n# bigtrain_sizes = list(range(300,401))\n# param_range = [300]\n# for i in bigtrain_sizes:\n#     if i % 10 == 0:\n#         param_range.append(int(i))\n\n\n# param_range = ('lbfgs', 'sgd', 'adam')\nparam_range = ('identity', 'logistic', 'tanh', 'relu')\n# param_range = ('constant', 'invscaling', 'adaptive')\n\nparam_range_len = np.arange(len(param_range))\n\nclf = MLPClassifier(batch_size=10, solver = 'lbfgs', hidden_layer_sizes = 300, verbose=False, early_stopping=True)\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\ntrain_scores, validation_scores = validation_curve(estimator= clf, X = x_train_, y = y_train_, param_name = 'activation', param_range = param_range, cv = cv, scoring = 'balanced_accuracy', verbose = 0)\ntrain_scores_mean = train_scores.mean(axis =1)\nvalidation_scores_mean = validation_scores.mean(axis = 1)\n# print(pd.Series(train_scores_mean, index = train_sizes))\n# print(pd.Series(validation_scores_mean, index = train_sizes))\nprint(validation_scores_mean)\n\nplt.style.use('seaborn')\nplt.plot(param_range, train_scores_mean, label = \"Training Error\")\nplt.plot(param_range, validation_scores_mean, label = \"Validation Error\")\nplt.ylabel('balanced_accuracy', fontsize = 14)\nplt.xlabel('Activation function', fontsize = 14)\nplt.title(\"Learning curves\", fontsize = 14)\nplt.legend()\nplt.ylim(0,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLPClassifier model with ideal parameters found using validation curves","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split, ShuffleSplit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, log_loss, f1_score\nfrom sklearn.model_selection import train_test_split, ShuffleSplit\nfrom sklearn.neural_network import MLPClassifier\n\nx_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size = .2, random_state = 1)\n\n\ndef getModel():\n    model = MLPClassifier(batch_size=10, solver = 'lbfgs', learning_rate = 'invscaling', activation = 'identity', hidden_layer_sizes = 300, verbose=False, early_stopping=True)\n    model.fit(x_train_,y_train_)\n    return model\n    \nmodel = getModel()\n\ncv = ShuffleSplit(n_splits=200, test_size=0.2, random_state=1)\nresults = cross_val_score(model, x_train_, y_train_, cv=cv)\n\nprint(results.mean)\nprint('Dummy model accuracy: %.3f%%' % dummy_acc)\n\n#basic predict - validation set\ny_pred1 = model.predict(x_val)\npred_accuracy_percentage1 = 100 * accuracy_score(y_val,y_pred1)\nprint('Validation set accuracy: %.3f%%' % pred_accuracy_percentage1)\n\n#basic predict - test set\ny_pred = model.predict(x_test)\npred_accuracy_percentage = 100 * accuracy_score(y_test,y_pred)\nprint('Test set accuracy: %.3f%%' % pred_accuracy_percentage)\n\n#ROC AUC metric\n# pred_auc = model.predict(x_test)\npred_auc = model.predict_proba(x_test)[:,1]\nacc = roc_auc_score(y_test, pred_auc)\nprint('AUC: %.3f%%' % (100*acc))\n\n#log loss metric\nlloss = log_loss(y_test,pred_auc, normalize = True)\nprint('Log loss: %.3f' % lloss)\n\n#f1_score\nf1 = f1_score(y_test,y_pred)\nprint('F1 Score: %.3f' % f1)\n\n#Confusion Matrix\nplt.figure()\nax = plt.axes()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot = True, fmt = 'd', ax = ax)\nax.set_title('Confusion Matrix of Loan Default Classifier')\nplt.show()\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}